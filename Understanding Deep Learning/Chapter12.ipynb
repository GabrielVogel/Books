{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "- Initially targeted at NLP\n",
    "- Goal is to design a network that can lear a text representation that is suitable for downstream tasks\n",
    "- Way of connect to related words in the text (**attention**)\n",
    "- Should extend across large text spans\n",
    "\n",
    "## Dot-product self attention\n",
    "- A self-attention block takes $N$ inputs each of dimension $D$ and outputs $N$ vectors of the same size. Each input represents a word or word fragment\n",
    "- First: Set of values are computed for each input $$v_m = \\beta_v + \\Omega_v x_m$$ where $\\Omega \\in \\mathbb{R}^{D \\times D}$\n",
    "- Second: \n",
    "  - The $n^{th}$ output of a self-attention block $sa_n[x_1,...,x_N] is a weighted sum of all values $v_1,...,v_N$ $$sa_n[x_1,...,x_N] = \\sum_{m=1}^N a[x_m,x_n]v_m$$\n",
    "  - The scalar weight $a[x_m,x_n]$ is the attention the $n^{th}$ output pays attention to input $x_m$\n",
    "  - The $N$ weights $a[*,x_n]$ are non-negative and sum to one\n",
    "## Attention weights\n",
    "- To compute the attention we apply two more linear transformations to the input $$q_n = \\beta_q + \\Omega_q x_n \\\\ k_m = \\beta_k + \\Omega_k x_m$$ where $q_n$ is the query and $k_m$ is the key.\n",
    "- Then, dot product between the queries and keys $$a[x_m,x_n] = softmax_m[k^T q_n] \\\\ = \\frac{exp[k_m^T,q_n]}{\\sum_{m'=1}^N exp[k_{m'}^T,q_n]}$$\n",
    "- Dot product returns a measure of similarity between queries and keys\n",
    "- Summary Self Attention\n",
    "  - $n^{th}$ output is a weighted sum of the same linear transformation $v_* = \\beta_v + \\Omega_v x_*$ applied to all inputs\n",
    "  - The weights depend on how similar $x_n$ is to the other inputs\n",
    "  - There is no activation function, but the mechanism is non-linear due to dot-product\n",
    "- Written in matrix form (X is the $D \\times N$ matrix of inputs)  $$ V[X] = \\beta_v \\mathbb{1}^T + \\Omega_v X \\\\ Q[X] = \\beta_q \\mathbb{1}^T + \\Omega_q X \\\\ K[X] = \\beta_k \\mathbb{1}^T + \\Omega_k X  \\\\ Sa[X] = V[X] \\cdot Softmax[K[X]^T Q[X]]$$\n",
    "\n",
    "## Extensions to self-attention\n",
    "- Positional encoding \n",
    "  - Absolute positional encoding\n",
    "    - Matrix $\\Pi$ is added to the input\n",
    "    - Each column of $\\Pi$ is different\n",
    "    - Can be chosen by hand or learned\n",
    "- Scaled self attention\n",
    "  $$Sa[X] = V \\cdot Softmax\\left[\\frac{K^T Q }{\\sqrt{D_q}}\\right]$$\n",
    "- Multiple heads\n",
    "  - Multiple self attention mechanisms\n",
    "  - For each query,key,value matrix for each head there is an parameter matrix\n",
    "  - $h^{th}$ self attention is $$Sa_h[X] = V_h \\cdot Softmax\\left[ \\frac{K_h^T Q_h}{\\sqrt{D_{qh}}} \\right]$$\n",
    "  - Normally, if there are $H$ heads and the input dimension is $D$, parameters will be size $D/H$\n",
    "  - Vertically concatenated and another linear transformation $\\Omega_c$ is applied to them $$MhSa[X] = \\Omega_c[Sa_1[X]^T, Sa_2[X]^T,...,Sa_H[X]^T]^T$$\n",
    "  - Necessary for performance\n",
    "\n",
    "## Transformers\n",
    "- Transformer mechanism\n",
    "  - Multi-head self attention\n",
    "    - Allow word representations to interact with each other\n",
    "  - Fully connected network that operates separately on each word\n",
    "  - Both units are residual networks\n",
    "  - Layer norm operation after both s.a and fcn\n",
    "    - Similar to batch norm, but uses statistics across the tokens within a single input to compute the statistics\n",
    "  - Can be described as $$X \\leftarrow X + MhSa[X] \\\\ X \\leftarrow LayerNorm[X] \\\\ x_n \\leftarrow x_n + mlp[x_n], \\forall n \\in \\{1,2...,N\\} \\\\ X \\leftarrow LayerNorm[X]$$\n",
    "\n",
    "## Transformers for NLP\n",
    "- Typical NLP pipeline starts with a tokenizer\n",
    "  - Splits text into word or word fragments\n",
    "  - Each of these is mapped to a word embedding\n",
    "  - These are passed through a series of transformers\n",
    "- Tokenization\n",
    "  - Using byte-pair encoding for example\n",
    "    - Greedily merges commonly occuring substrings based on their frequency\n",
    "- Embeddings\n",
    "  - Each token is mapped to an embedding\n",
    "  - Store in a $\\Omega_e \\in \\mathbb{R}^{D \\times |V|}$ matrix\n",
    "  - First, the $N$ input tokens are mapped to a $T^{|V| \\times N}$ matrix, where the $n_th$ column correspond to the $n_th$ token and it's a one-hot vector\n",
    "  - The input embeddings are computed as $X = \\Omega_e T$, and $\\Omega_e$ is learned as any other parameter.\n",
    "- Transformer model\n",
    "  - Embedding matrix $X$ is passed through a series of $K$ transformers\n",
    "  - Encoder\n",
    "    - Transform text embeddings into a representation that can support a variety of tasks\n",
    "  - Decoder\n",
    "    - Predicts the next token to continue the input text\n",
    "- **Encoder-decoder** models are used in sequence-to-sequence tasks, like machine translation\n",
    "\n",
    "## Encoder model: BERT\n",
    "- Uses 24 transformers blocks\n",
    "- 1024-word embedding dimensions\n",
    "- 16 heads in self attention\n",
    "- Exploit transfer learning\n",
    "- During pre-training, parameters are learned using self-supervised learning\n",
    "- Fine-tuning: Resulting network is adapted to solve a downstream task\n",
    "- **Pre-training**\n",
    "  - Self supervised task: Predicting missing words from the text\n",
    "  - Max input length: 512 tokens\n",
    "  - Predicting missing words allows to learn some syntax\n",
    "    - Red is often found before house or car, but never before a verb like shout\n",
    "    - Degree of understanding in this type of model can ever have is limited\n",
    "  - Also uses a secondary task if two sentences are adjacents to each other\n",
    "- **Fine tuning**\n",
    "  - Text classification\n",
    "    - Uses the $<cls>$ token to a classification layer\n",
    "  - Word classification\n",
    "    - NER(Named Entity Recognition): Classify each word as an entity type\n",
    "      - Each input embedding is mapped to an $E \\times 1$ vector where each of the $E$ entries correspond to an entity type\n",
    "      - Passed through an softmax to create probability for each class\n",
    "  \n",
    "## Decoder model: GPT3\n",
    "- Generate next token in the sequence\n",
    "- Auto-regressive language model\n",
    "- Maximize log probability of the input text under the autoregressive model\n",
    "- Set attention to the answer and the right context to zero (masked self-attention)\n",
    "- LLMs\n",
    "- Can perform many tasks without fine-tuning\n",
    "- Are few shot learners\n",
    "  - Can learn to do novel tasks based on just a few examples\n",
    "\n",
    "## Encoder-Decoder: Machine Translation\n",
    "- Translation between languages is a sequence-to-sequence task\n",
    "- Encoder:\n",
    "  - Receives sentence in a language and creates an output representation for each token\n",
    "- Decoder:\n",
    "  - Receives the ground truth translation and passes through a sequence of transformers that use masked self-attention and predicts the following word at every position\n",
    "  - Conditioned on the previous output words *and* the source language text\n",
    "- Encoder-decoder cross attention\n",
    "\n",
    "## Transformers for long sequences\n",
    "- Computational complexity scales quadractically with the length of the sequence\n",
    "\n",
    "## Transformers for images\n",
    "- ImageGPT\n",
    "  - Autoregressive model of image pixels that ingest a partial image and predicts the subsequent pixel value\n",
    "  - Due to size, operates on 64x64 shaped images\n",
    "  - Learns that each pixel has a close relationship with nearby pixels\n",
    "- Vision Transformers (ViT)\n",
    "  - Divides the image into 16x16 patches\n",
    "  - Each patch is mapped by a linear transformation and feed to transformer blocks\n",
    "  - Standard 1D positional encodings are learned\n",
    "  - Encoder model with \\<cls\\> token\n",
    "    - Mapped via a network layer that create activations that are feed to a softmax layer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
