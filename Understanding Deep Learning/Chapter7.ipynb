{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and Initialization\n",
    "## Backpropagation\n",
    "- Derivatives of a loss tell us how the loss changes when we make a small change to the parameters\n",
    "- Backpropagation algorithm\n",
    "  - Compute partial derivative with respect to each parameter\n",
    "  - Consists of\n",
    "    - Forward pass: Compute and store series of intermediate values and the network output\n",
    "    - Backward pass: Derivatives of each parameter, starting at the end of the network\n",
    "  - Deep neural network $f[x_i,\\phi]$, $K$ hidden layers with ReLU and individual loss term $l_{i} = l[f[x_{i},\\phi],y_{i}]$\n",
    "  - Goal: Compute partial derivatives $\\frac{\\partial l_{i}}{\\partial \\Omega_{k}}$ and $\\frac{\\partial l_{i}}{\\partial \\beta_{k}}$ with respects to biases $\\beta_{k}$ and weights $\\Omega_{k}$\n",
    "  - Forward pass:\n",
    "    - For each $k$, compute <br>  $ f_{0} = \\beta_{0} + \\Omega_{0}x_{i}$ <br> $h_{k} = a[f_{k-1}], k \\in \\{1,2,3...,K\\} $ <br> $f_{k} = \\beta_{k} + \\Omega_{k}h_{k}, k \\in \\{1,2,3...,K\\} $\n",
    "  - Backward pass:\n",
    "    - Start with the derivative $\\frac{\\partial l_i}{\\partial f_K}$ of the loss $l_{i}$ with respect to network output $f_{K}$ and work backthrough the network <br>\n",
    "    $$\\frac{\\partial l_i}{\\partial \\beta_k} = \\frac{\\partial l_i}{\\partial f_k}, k \\in \\{K,K-1,...,1\\}$$\n",
    "    $$\\frac{\\partial l_i}{\\partial \\Omega_k} = \\frac{\\partial l_i}{\\partial f_k}h_k^T, k \\in \\{K,K-1,...,1\\}$$\n",
    "    $$\\frac{\\partial l_i}{\\partial f_{k-1}} = \\mathbb{I}[f_{k-1} > 0] \\odot \\left(\\Omega_{k}^T \\frac{\\partial l_i}{\\partial f_k}\\right), k \\in \\{K,K-1,...,1\\}$$\n",
    "  - Calculate those derivate for every training example in the batch and sum them together to retrieve the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter initialization\n",
    "- If we initialize parameters from a $\\mathbb{N}(0,\\sigma^2)$\n",
    "  - If variance is too small, the magnitudes of the pre-activations will become smaller and smaller. Can lead to **vanishing gradient**\n",
    "  - If the variance is too large, the magnitudes of the pre-activation will become larger and larger. Can lead to **exploding gradient**\n",
    "- He initialization  $$\\sigma_{\\Omega}^2 = \\frac{2}{D_h}$$\n",
    "- If the weight matrix is not squared, then we can use the mean $(D_h + D_h')/2$ as a proxy, which yields $$\\sigma_{\\Omega}^2 = \\frac{4}{D_h + D_h'}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0, loss 20.080\n",
      "Epoch     1, loss 9.577\n",
      "Epoch     2, loss 9.208\n",
      "Epoch     3, loss 8.933\n",
      "Epoch     4, loss 8.582\n",
      "Epoch     5, loss 8.016\n",
      "Epoch     6, loss 7.561\n",
      "Epoch     7, loss 7.298\n",
      "Epoch     8, loss 7.198\n",
      "Epoch     9, loss 6.812\n",
      "Epoch    10, loss 6.666\n",
      "Epoch    11, loss 6.227\n",
      "Epoch    12, loss 5.782\n",
      "Epoch    13, loss 5.658\n",
      "Epoch    14, loss 5.520\n",
      "Epoch    15, loss 5.296\n",
      "Epoch    16, loss 5.148\n",
      "Epoch    17, loss 4.929\n",
      "Epoch    18, loss 4.813\n",
      "Epoch    19, loss 4.626\n",
      "Epoch    20, loss 4.521\n",
      "Epoch    21, loss 4.416\n",
      "Epoch    22, loss 4.354\n",
      "Epoch    23, loss 4.317\n",
      "Epoch    24, loss 4.293\n",
      "Epoch    25, loss 4.261\n",
      "Epoch    26, loss 4.207\n",
      "Epoch    27, loss 4.194\n",
      "Epoch    28, loss 4.137\n",
      "Epoch    29, loss 4.106\n",
      "Epoch    30, loss 4.048\n",
      "Epoch    31, loss 4.021\n",
      "Epoch    32, loss 4.005\n",
      "Epoch    33, loss 3.966\n",
      "Epoch    34, loss 3.940\n",
      "Epoch    35, loss 3.931\n",
      "Epoch    36, loss 3.911\n",
      "Epoch    37, loss 3.904\n",
      "Epoch    38, loss 3.866\n",
      "Epoch    39, loss 3.860\n",
      "Epoch    40, loss 3.838\n",
      "Epoch    41, loss 3.832\n",
      "Epoch    42, loss 3.823\n",
      "Epoch    43, loss 3.815\n",
      "Epoch    44, loss 3.810\n",
      "Epoch    45, loss 3.803\n",
      "Epoch    46, loss 3.796\n",
      "Epoch    47, loss 3.791\n",
      "Epoch    48, loss 3.774\n",
      "Epoch    49, loss 3.761\n",
      "Epoch    50, loss 3.751\n",
      "Epoch    51, loss 3.747\n",
      "Epoch    52, loss 3.741\n",
      "Epoch    53, loss 3.737\n",
      "Epoch    54, loss 3.734\n",
      "Epoch    55, loss 3.732\n",
      "Epoch    56, loss 3.727\n",
      "Epoch    57, loss 3.723\n",
      "Epoch    58, loss 3.720\n",
      "Epoch    59, loss 3.717\n",
      "Epoch    60, loss 3.711\n",
      "Epoch    61, loss 3.708\n",
      "Epoch    62, loss 3.707\n",
      "Epoch    63, loss 3.705\n",
      "Epoch    64, loss 3.703\n",
      "Epoch    65, loss 3.702\n",
      "Epoch    66, loss 3.701\n",
      "Epoch    67, loss 3.699\n",
      "Epoch    68, loss 3.697\n",
      "Epoch    69, loss 3.696\n",
      "Epoch    70, loss 3.693\n",
      "Epoch    71, loss 3.693\n",
      "Epoch    72, loss 3.692\n",
      "Epoch    73, loss 3.691\n",
      "Epoch    74, loss 3.691\n",
      "Epoch    75, loss 3.690\n",
      "Epoch    76, loss 3.689\n",
      "Epoch    77, loss 3.688\n",
      "Epoch    78, loss 3.688\n",
      "Epoch    79, loss 3.687\n",
      "Epoch    80, loss 3.686\n",
      "Epoch    81, loss 3.686\n",
      "Epoch    82, loss 3.685\n",
      "Epoch    83, loss 3.685\n",
      "Epoch    84, loss 3.684\n",
      "Epoch    85, loss 3.684\n",
      "Epoch    86, loss 3.684\n",
      "Epoch    87, loss 3.683\n",
      "Epoch    88, loss 3.683\n",
      "Epoch    89, loss 3.683\n",
      "Epoch    90, loss 3.682\n",
      "Epoch    91, loss 3.682\n",
      "Epoch    92, loss 3.682\n",
      "Epoch    93, loss 3.681\n",
      "Epoch    94, loss 3.681\n",
      "Epoch    95, loss 3.681\n",
      "Epoch    96, loss 3.681\n",
      "Epoch    97, loss 3.681\n",
      "Epoch    98, loss 3.680\n",
      "Epoch    99, loss 3.680\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# define input size, hidden layer size, output size\n",
    "D_i, D_k, D_o = 10, 40, 5\n",
    "# create model with two hidden layers\n",
    "model = nn.Sequential(\n",
    "nn.Linear(D_i, D_k),\n",
    "nn.ReLU(),\n",
    "nn.Linear(D_k, D_k),\n",
    "nn.ReLU(),\n",
    "nn.Linear(D_k, D_o))\n",
    "# He initialization of weights\n",
    "def weights_init(layer_in):\n",
    "    if isinstance(layer_in, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(layer_in.weight)\n",
    "        layer_in.bias.data.fill_(0.0)\n",
    "model.apply(weights_init)\n",
    "# choose least squares loss function\n",
    "criterion = nn.MSELoss()\n",
    "# construct SGD optimizer and initialize learning rate and momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
    "# object that decreases learning rate by half every 10 epochs\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "# create 100 random data points and store in data loader class\n",
    "x = torch.randn(100, D_i)\n",
    "y = torch.randn(100, D_o)\n",
    "data_loader = DataLoader(TensorDataset(x,y), batch_size=10, shuffle=True)\n",
    "# loop over the dataset 100 times\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0.0\n",
    "    # loop over batches\n",
    "    for i, data in enumerate(data_loader):\n",
    "    # retrieve inputs and labels for this batch\n",
    "        x_batch, y_batch = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        pred = model(x_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # SGD update\n",
    "        optimizer.step()\n",
    "    # update statistics\n",
    "        epoch_loss += loss.item()\n",
    "    # print error\n",
    "    print(f'Epoch {epoch:5d}, loss {epoch_loss:.3f}')\n",
    "    # tell scheduler to consider updating learning rate\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
