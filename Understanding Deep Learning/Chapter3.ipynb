{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Networks\n",
    "- Shallow neural networks are functions $y=f[x,\\phi]$ that map multivariate inputs to multivariate outputs.\n",
    "- $y=f[x,\\phi]$ <br> = $\\phi_{0} + \\phi_1 a[\\theta_{10} + \\theta_{11}x] + \\phi_2 a[\\theta_{20} + \\theta_{21}x] + \\phi_3 a[\\theta_{30} + \\theta_{31}x] $\n",
    "- First, computes three linear functions on the input data $\\theta_{i0} + \\theta_{i1}x$\n",
    "- Second, pass each through an activation function\n",
    "- Weight the tree resulting activations with $\\phi_{i}$ and add an offset $\\phi_{0}$\n",
    "- Activation function\n",
    "  - $relu(z) = max(z,0)$\n",
    "\n",
    "## Intuition\n",
    "$\\phi_{0} + \\phi_1 a[\\theta_{10} + \\theta_{11}x] + \\phi_2 a[\\theta_{20} + \\theta_{21}x] + \\phi_3 a[\\theta_{30} + \\theta_{31}x]$\n",
    "  - Represents a family of continuous piecewise linear functions with up to four linear regions\n",
    "  - Hidden units\n",
    "    - $ h_1 = \\theta_{10} + \\theta_{11}x$ <br> $ h_2 = \\theta_{20} + \\theta_{21}x$ <br> $ h_3 = \\theta_{30} + \\theta_{31}x $ <br>\n",
    "  - Output:\n",
    "    - $y = \\phi_{1}h_1 + \\phi_{2}h_2 + \\phi_{3}h_3 + \\phi_{0}$\n",
    "  - Each linear region corresponds to a different activation pattern\n",
    "  - If its clipped, its inactive, else active\n",
    "  - Each hidden unit contributes one \"joint\" to the function, so with three hidden units, there can be four linear regions\n",
    "\n",
    "## Universal Approximation Theorem\n",
    "- Case with $D$ hidden units, where $h_d = a[\\theta_{d0} + \\theta_{d1}x]$, and are combined to create $y = \\phi_{0} + \\sum_{d}^D \\phi_{d}h_d$\n",
    "- Network capacity: Number of hidden units\n",
    "- With RELU activation functions, the output of a network with $D$ hidden units has at most $D$ joints, so a piecewise linear function has $D+1$ linear regions\n",
    "- With enough capacity, a shallow network can describe any continuous $1D$ function defined on a compact subset of the real line to arbitrary precision\n",
    "- With more regions, they represent smaller sections of the functio\n",
    "\n",
    "## Multivariate input\n",
    "- With $[x_1, x_2]$ as inputs, $h_{i} = a[\\theta_{i0} + \\theta_{i1}x_{1} + \\theta_{i2}x_{2}]$\n",
    "- Each input has one slope\n",
    "- Each hidden unit receives a linear combination of the inputs\n",
    "- Linear regions become convex **polytopes** in the multi-dimensional input space\n",
    "  - **polytope**: \n",
    "    - 2d polytope is a polygon\n",
    "    - 1d polytope is a line\n",
    "- Input dimensions grow, linear regions increase rapidly\n",
    "  - Each hidden unit defines a hyperplane where the unit is active and where its not\n",
    "  - For two dimensions, divides space in 4 quadrants\n",
    "  - For three, into 8\n",
    "  - For $n$, $2^n$\n",
    "\n",
    "## General Case\n",
    "- Maps multidimensional input $x \\in R^{Di}$ to multidimensional output $y \\in R^{Do}$\n",
    "- Each hidden unit is computed as $h_{d} = a[\\theta_{d0} + \\sum_{i}^{Di} \\theta_{di}x_{i}]$\n",
    "- Linear combined to create the output $y_j = \\phi_{j0} + \\sum_{d=1}^{Di} \\phi_{jd}h_d$\n",
    "- The activation function permits the model to describe non linear functions of $x$ and to do it, it must be nonlinear itself\n",
    "- RELU:\n",
    "  - Network divides the input space into convex polytopes defined by the intersection of hyperplanes computed by the \"joints\" of RELU\n",
    "  - Each convex polytope contais a different linear function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
