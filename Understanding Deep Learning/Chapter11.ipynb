{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks\n",
    "- In standard neural network, each layer consists of a linear transformation followed by an activation function\n",
    "- In convolutional network, each layer consists of a set of convolutions followed by an activation function\n",
    "- Limitations of sequential processing\n",
    "  - Image classification performance decreases as more layers are added\n",
    "  - Problem is in training deeper networks rather than the inability of deeper networks to generalize\n",
    "  - Shattered gradient phenomenon\n",
    "    - Nearby gradients are correlated for shallow networks, but this correlation drops to zero for deeper ones\n",
    "## Residual connections\n",
    "- Residual (skip) connections are branches in the computational path, where the input to each network layer is added back to the output $$h_1 = x + f_1[x,\\phi_1] \\\\ h_2 = h_1 + f_2[h1,\\phi_2] \\\\ h_3 = h_2 + f_3[h_2,\\phi_3]$$\n",
    "- Each additive combination of the input and the processed output is known as *residual layer* \n",
    "- Can see as $$ y = x + f_1[x] + \\\\ f_2[x + f_1[x]] + \\\\ f_3[x + f_2[x+f_1[x]] + f_1[x]]$$\n",
    "- It's a sum of the input and three smaller networks\n",
    "- Can be seen as an ensemble of these smaller networks whose outputs are summed to compute the result\n",
    "- Typical to start the network with a linear transformation instead of a residual block\n",
    "## Exploding gradients in residual networks\n",
    "- Do not need to worry about vanishing gradients, because each layer contributes directly to the output\n",
    "- Still suffers from exploding gradients\n",
    "- Way to solve that is through **Batch normalization**\n",
    "- Introduce an exponential increase in variance of the activations during the forward propagation\n",
    "- **Batch normalization**\n",
    "  - Shifts and rescales each activation $h$ such that its mean and variance across the batch $\\mathbb{B}$ becomes values that are learned through training\n",
    "  - Process\n",
    "    - Mean $m_h$ and standard deviation $s_h$ are computed\n",
    "    - Use theses statistics to standardize the batch $$h_i \\leftarrow \\frac{h_i - m_h}{s_h + \\epsilon}, \\forall i \\in \\mathbb{B}$$\n",
    "    - Then, normalized value is scaled by $\\gamma$ and shifted by $\\delta$ $$h_i \\leftarrow \\gamma h_i + \\delta$$\n",
    "    - Both these quantities are learned through training\n",
    "  - Is applied independently to each hidden unit\n",
    "  - Computed over both batch and spatial position \n",
    "    - $K$ layers and $C$ channels, $KC$ offsets and $KC$ scales\n",
    "  - At test time, we do not have batch to gather statistics\n",
    "    - $m_h$ and $s_h$ are calculated across all training set and frozen in the final network\n",
    "  - Benefits\n",
    "    - Loss surface is smoother\n",
    "    - Makes network invariant to rescaling the weights and biases\n",
    "    - Stable forward propagation\n",
    "    - Higher learning rates\n",
    "      - Can use higher learning rates, that improves test performance\n",
    "    - Regularization\n",
    "      - Adding noise helps in generalization\n",
    "      - Batch normalization injects noise because of the batch statistics\n",
    "## Common residual architectures\n",
    "- ResNet\n",
    "  - Each residual block contains\n",
    "    - BatchNorm\n",
    "    - ReLU\n",
    "    - Convolutional layer\n",
    "  - Followed by the same sequence again and added by the input\n",
    "- DenseNet\n",
    "  - Concatenate modified and original signals\n",
    "  - Input to a layer comprises of the concatenated output from **all** the previous layers\n",
    "- U-Nets\n",
    "  - Earlier representations are concatenated to later ones\n",
    "  - **Completely convolutional**\n",
    "## Why do nets with residual connections perform so well?\n",
    "- Allow much deeper networks to be trained\n",
    "- Residual connections add value on their own"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
