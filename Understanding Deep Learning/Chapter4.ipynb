{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "- For some functions, the required number of hidden units it's impractically large\n",
    "- Deep networks can produce many more linear regions than shallow networks \n",
    "- From a pratical standpoint, they can be used to describe a broader family of functions\n",
    "\n",
    "## Composing Neural Networks\n",
    "- Composing two networks, such that the output of one becomes the input of the second\n",
    "- First\n",
    "  - $h_{1} = a[\\theta_{10} + \\theta_{11}x]$ <br> $h_{2} = a[\\theta_{20} + \\theta_{21}x]$ <br> $h_{3} = a[\\theta_{30} + \\theta_{31}x]$ <br> $y=\\phi_{0} + \\phi_{1}h_1 + \\phi_{2}h_2 + \\phi_{3}h_3$\n",
    "- Second\n",
    "  -  - $h_{1}' = a[\\theta_{10}' + \\theta_{11}'y]$ <br> $h_{2}' = a[\\theta_{20}' + \\theta_{21}'y]$ <br> $h_{3}' = a[\\theta_{30}' + \\theta_{31'}y]$ <br> $y'=\\phi_{0}' + \\phi_{1}'h_1' + \\phi_{2}'h_2' + \\phi_{3}'h_3'$\n",
    "- Number of linear regions produced is greather than for a shallow network with $6$ hidden units\n",
    "- Increase to $9$ linear regions (3x3)\n",
    "- The first network folds the input space $x$ back onto itself so that multiple inputs generate the same output\n",
    "- Then, the second network applies a function, which is replicated at all points that were folded on top of one another\n",
    "\n",
    "## Deep Neural Networks\n",
    "- Case with 2 hidden layers and 3 hidden units\n",
    "  - The hidden units from the first layer are usual linear functions followed by ReLU.\n",
    "  - Pre-activations of the second layer\n",
    "    - Three linear functions on the hidden units\n",
    "  - At the second hidden layer, another ReLU function is applied to each function which clips and adds new joints to each\n",
    "  - Final output is a linear combination of these hidden units\n",
    "## Hyperparameters\n",
    "- Modern networs might have thousands of hidden units and hundreds of layers\n",
    "- Width: Number of hidden units\n",
    "- Depth: Number of hidden layers\n",
    "- Number of layers as $K$ and the number of hidden units in each layer as $D_{1}, D_{2},...$\n",
    "\n",
    "## Matrix Notation\n",
    "- $\\begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{bmatrix}$ = $a[\\begin{bmatrix} \\theta_{10} \\\\ \\theta_{20} \\\\ \\theta_{30} \\end{bmatrix} + \\begin{bmatrix} \\theta_{11} \\\\ \\theta_{21} \\\\ \\theta_{31} \\end{bmatrix}x]$\n",
    "- $\\begin{bmatrix} h_1' \\\\ h_2' \\\\ h_3' \\end{bmatrix}$ = $a[\\begin{bmatrix} \\psi_{10} \\\\ \\psi_{20} \\\\ \\psi_{30} \\end{bmatrix} + \n",
    "  \\begin{bmatrix} \\psi_{11} & \\psi_{12} & \\psi_{13} \\\\\n",
    "  \\psi_{21} & \\psi_{22} & \\psi_{23} \\\\\n",
    "  \\psi_{31} & \\psi_{32} & \\psi_{33} \\end{bmatrix} \\begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{bmatrix}]$\n",
    "- $y' = \\phi_{0}' + \\begin{bmatrix} \\phi_1' & \\phi_2' & \\phi_3'\\end{bmatrix} \\begin{bmatrix} h_1' \\\\ h_2' \\\\ h_3' \\end{bmatrix}$\n",
    "\n",
    "- OR, \n",
    "  - $h = a[\\theta_{0} + \\theta x]$\n",
    "  - $h' = a[\\psi_{0} + \\Psi h]$\n",
    "  - $y' = \\phi_{0}' + \\phi'h'$\n",
    "- Notation:\n",
    "  - Vector of hidden units at layer $k$ as $h_k$\n",
    "  - Vector of biases that contributes to hidden layer $k+1$ as $\\Beta_{k}$ and the weights(slopes) that are applied to the $k^{th}$ layer as $\\Omega_{k}$\n",
    "  - A general deep network can be written as \n",
    "    - $h_{k} = a[\\Beta_{k-1} + \\Omega_{k-1}h_{k-1}]$\n",
    "- In the $k^{th}$ layer\n",
    "  - $\\Beta_{k-1}$ size will be $D_{k}$\n",
    "  - $\\Omega_{k}$ has size $D_{k+1}D_{k}$\n",
    "\n",
    "## Shallow vs Deep\n",
    "- Ability to approximate different functions\n",
    "  - Both can approximate any function (Universal Approximation Theorem)\n",
    "- Number of linear regions\n",
    "  - Shallow:\n",
    "    - With one input, one output and $D$ units, can create up to $D+1$ linear regions and is defined by $3D+1$ parameters\n",
    "      - $2D$ between input and hidden (weight and bias pair), $D$ for the hidden and output and + 1 for the bias \n",
    "  - Deep\n",
    "    - With one input, one output, $K$ layers and  $D > 2$ hidden units\n",
    "      - up to $(D+1)^K$ linear regions\n",
    "      - $3D + 1 + (K-1)D(D+1)$ params\n",
    "        - $3D+1$ like the shallow\n",
    "        - $(K-1)$ (removing one of the layer (input or output)) times $D(D-1)$ that is the matrix size between capacities in consecutive layers\n",
    "  - Deep networks produce many more linear regions per parameters\n",
    "- Depth Efficiency\n",
    "  - Some functions can be approximated much more efficiently with deep networks\n",
    "- Training/Generalization\n",
    "  - Deep networks are easier to fit\n",
    "  - Maybe because overparametrized deep models have a large family of functions that are easy to find\n",
    "  - Deep networks seem to generalize more\n",
    "- In practice, the best results are achieved using using deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
