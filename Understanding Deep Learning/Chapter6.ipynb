{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Models\n",
    "- Process\n",
    "  - Choose random initial values\n",
    "  - Compute the derivatives of the loss with respect to the parameters\n",
    "  - Adjust the parameters to decrease the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "- Goal of an optimization algorithm is to find parameters $\\phi$ that minimizes the training loss\n",
    "- Starts with parameters $\\phi = \\begin{bmatrix} \\phi_{0}, \\phi_{1},...,\\phi_{n} \\end{bmatrix}^T$\n",
    "  - Step 1: Compute the derivative with respect to the parameters $$\\frac{\\partial L}{\\partial \\phi} = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\phi_{0}} \\\\ \\frac{\\partial L}{\\partial \\phi_{1}} \\\\ . \\\\ .  \\\\ . \\\\   \\frac{\\partial L}{\\partial \\phi_{N}}        \\end{bmatrix}$$\n",
    "    - Computes the uphill direction of the loss function \n",
    "  - Step 2: Update the parameters according to the rule $$\\phi = \\phi - \\alpha . \\frac{\\partial L}{\\partial \\phi}$$\n",
    "    - Moves a small distance $\\alpha$ downhill. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "- Has a well defined global minimum\n",
    "- They are **convex**\n",
    "  - No cord (line segment between two points on the surface) intersects the function\n",
    "  - Implies that we are bound to reach the minimum\n",
    "- Shallow and Deep Neural Network are non-convex\n",
    "- Local minima: Point where the gradient is $0$, but there are other points where the loss is smaller\n",
    "- No way of knowing if there is a better solution elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "- Adding some noise to the gradient at each step\n",
    "- At each iteration, the algorithm chooses a random subset of the data and computes the gradient for this batch alone\n",
    "- Drawn without replacement\n",
    "- Works through all the training examples, batch by batch\n",
    "- One full pass through the data is an *epoch*\n",
    "- Less computationally expensive\n",
    "- Can escape local minima\n",
    "- Reduces the chance of getting stuck at saddle points\n",
    "- Learning rate schedule: $\\alpha$ starts with a high value and decreases by a constant factor every $N$ epochs\n",
    "  - Early stages: Exploring spaces\n",
    "  - Later stages: More concerned with fine tuning the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "$$m_{t+1} \\leftarrow \\beta m_{t} + (1-\\beta)\\sum_{i \\in B_{t}} \\frac{\\partial l_{i}[\\phi_{t}]}{\\partial \\phi}$$\n",
    "$$\\phi_{t+1} \\leftarrow \\phi_{t} - \\alpha m_{t+1}$$\n",
    "- Gradient step is an infinite weighted sum of all the previous gradients, where the weights get smaller as we move back in time\n",
    "\n",
    "### Nesterov accelerated momentum\n",
    "$$m_{t+1} \\leftarrow \\beta m_{t} + (1-\\beta)\\sum_{i \\in B_{t}} \\frac{\\partial l_{i}[\\phi_{t} - \\alpha m_{t}]}{\\partial \\phi}$$\n",
    "$$\\phi_{t+1} \\leftarrow \\phi_{t} - \\alpha m_{t+1}$$\n",
    "- Computes the gradient at the predicted point instead of the current point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM\n",
    "- Normalize gradients so that we move a fixed distance at each direction\n",
    "$$m_{t+1} = \\frac{\\partial L[\\phi_{t}]}{\\partial \\phi}$$\n",
    "$$v_{t+1} = \\left( \\frac{\\partial L[\\phi_t]}{\\partial \\phi}  \\right)^2$$\n",
    "- Update rule $$\\phi_{t+1} = \\phi_{t} - \\alpha  \\frac{m_{t+1}}{\\sqrt{v_{t+1}} + \\epsilon}$$\n",
    "- $\\epsilon$ prevents division by 0\n",
    "- Moves a fixed direction $\\alpha$ along each coordinate\n",
    "- ADAM takes this idea and adds momentum to both $$m_{t+1} = \\beta m_{t} + (1-\\beta) \\frac{\\partial L[\\phi_{t}]}{\\partial \\phi}$$  $$v_{t+1} = \\gamma v_{t} + (1-\\gamma)  \\left( \\frac{\\partial L[\\phi_t]}{\\partial \\phi}  \\right)^2$$\n",
    "- Modify this statistics by $$\\tilde{m}_{t+1} \\leftarrow \\frac{m_{t+1}}{1-\\beta^{t+1}}$$ $$\\tilde{v}_{t+1} = \\frac{v_{t+1}}{1-\\gamma^{t+1}}$$\n",
    "- Parameters modified as $$\\phi_{t+1} = \\phi_{t} - \\alpha \\frac{\\tilde{m}_{t+1}}{\\sqrt{\\tilde{v}_{t+1}} + \\epsilon}$$\n",
    "- Normally done in mini-batches\n",
    "- Less sensitive to initial learning rates\n",
    "- It doesnt need complex learning rate schedules\n",
    "- Balance out changes depending on the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
