{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "- Loss function: Returns a single number that describes the mismatch between the model predictions and the ground truth\n",
    "\n",
    "## Maximum Likelihood\n",
    "- Consider a model computing a conditional probability $Pr(y|x)$. The loss encourages each training output $y_i$ to have a high probability distribution over its input $x_i$.\n",
    "- How to compute\n",
    "  - Choose a parametric distribution\n",
    "  - Use the network to compute it's parameters\n",
    "$$\\hat{\\phi} = argmax_{\\phi}\\left[\\prod_{i=1}^I Pr(y_{i}|x_{i})\\right]$$\n",
    "$$= argmax_{\\phi}\\left[\\prod_{i=1}^I Pr(y_{i}|f[x_{i},\\theta])\\right]$$\n",
    "- Two assumptions are made\n",
    "  - The inputs are independent, so it can be multiplied\n",
    "  - Identically distributed (same probability distribution for all data points)\n",
    "  - This is the *i.i.d* assumption\n",
    "- Can be change to log-likelihood, and becomes $$\\hat{\\phi} = argmax_{\\phi}\\left[\\sum_{i=1}^I log[Pr(y_{i}|f[x_{i},\\theta]])\\right]$$\n",
    "  - Better to treat it as a sum, multiplying small numbers many times leads to underflow\n",
    "- Model fitting are framed in terms of minimization, so $$\\hat{\\phi} = argmin_{\\phi}\\left[-\\sum_{i=1}^I log[Pr(y_{i}|f[x_{i},\\theta]])\\right]$$\n",
    "\n",
    "## Univariate regression\n",
    "- Univariate normal $$Pr(y|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left[\\frac{(y-\\mu)^2}{2\\sigma^2}\\right]$$\n",
    "- Replacing the univariate normal in the negative log likelihood, eventually it yields $$L[\\phi] = \\sum_{i=1}^I (y_{i} - f[x_{i},\\phi])^2$$\n",
    "- In the above case, variance is assumed to be constant\n",
    "- **heteroscedastic**: When the uncertainty of the data varies as a function of the input\n",
    "  - Train a network to predict the variance\n",
    "  - Compute it as $\\sigma^2 = f_2[x,\\phi]^2$, as variance can have only positive values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "- Bernoulli distribution $$Pr(y|\\lambda) = (1-\\lambda)^{1-y} \\lambda^y$$\n",
    "- Network to predict $\\lambda$\n",
    "- Must pass the output through a sigmoid function that maps $\\mathbb{R}$ to $[0,1]$. $$sig[z] = \\frac{1}{1+exp[-z]}$$\n",
    "- Yields binary cross entropy loss $$L[\\phi] = \\sum_{i}^I -(1-y_{i})log[1-sig[f[x_i,\\phi]]] - y_{i}log[sig[f[x_i,\\phi]]]$$\n",
    "\n",
    "\n",
    "## Multiclass\n",
    "- Categorical distribution $Pr(y=k) = \\lambda_{k}$\n",
    "- Constrained to be between $[0,1]$ and sum up to $1$\n",
    "  - Softmax function is used to ensure this $$softmax_k[z] = \\frac{exp[z_k]}{\\sum_{k'=1}^K exp[z_k']}$$\n",
    "- Categorical cross entropy $$-\\sum_{i}^I \\left(f_{yi}[x_{i},\\phi] - log\\left[\\sum_{k'=1}^K exp[f_k'[x_{i},\\phi]]           \\right]          \\right)$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
