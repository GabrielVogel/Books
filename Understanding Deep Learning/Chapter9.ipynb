{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "- Technique that tries to reduce the generalization gap between training and test set performance\n",
    "\n",
    "## Explicit Regularization\n",
    "\n",
    "$$\\hat{\\phi} = argmin_{\\phi} \\left[\\sum_{i=1}^I l_i[x_i,y_i] + \\lambda g[\\phi]    \\right]$$\n",
    "\n",
    "- $g[\\phi]$ is a function that returns a scalar that takes a larger value when the parameters are less prefered. $\\lambda$ is a positive scalar that controls the contribution of this term to the loss function.\n",
    "### Probabilistic Interpretation\n",
    "- Regularization can be considered as a prior that represents knowledge about the parameters before seeing the data\n",
    "\n",
    "$$\\hat{\\phi} = argmin_{\\phi} \\left[\\prod_{i=1}^I Pr(y_i|xi_,\\phi)Pr(\\phi)   \\right]$$\n",
    "\n",
    "### L2 regulariztion\n",
    "$$\\hat{\\phi} = argmin_{\\phi} \\left[\\sum_{i=1}^I l_i[x_i,y_i] + \\lambda \\sum_{j} \\phi_j^2    \\right]$$\n",
    "- Penalizes the sum of squares of the parameter values  \n",
    "- Correspondes to $N(0,1)$ prior on the parameters\n",
    "- Weight decay: Applied only to weights, not biases\n",
    "- Effects\n",
    "  - If overfitting, increase regularization\n",
    "  - If is underfitting, regularization might be too strong\n",
    "- When the model is over-parametrized, some of the extra model capacity will describe areas with no training data\n",
    "\n",
    "## Implicit Regularization\n",
    "- Neither gradient descent or stochastic gradient descent moves neutrally to the minimum of the loss function, some solutions are prefered over others\n",
    "- Implict regularization due to gradient descent may be responsible for the observation that full batch gradient descent generalizes better with larger step sizes\n",
    "- SGD generalizes better than gradient descent and smaller batch sizes generally perform better than larger ones\n",
    "\n",
    "## Heuristics to improve performance\n",
    "- Early stopping\n",
    "  - Stopping the training procedure before it has fully converged\n",
    "  - Can reduce overfitting by stopping from capturing the noise of the data\n",
    "  - Similar effect to explict L2\n",
    "  - Single parameter: Number of steps after which learning is terminated\n",
    "    - Chosen using validation set\n",
    "- Ensembling\n",
    "  - Build several models and averaging their predictions\n",
    "  - Combined by:\n",
    "    - Mean of the outputs (regression) or median\n",
    "    - Mean of softmax outputs (multiclass ) or most frequent class in predictins\n",
    "  - How to train different models\n",
    "    - Different random initializations\n",
    "    - Bootstrap samples of the data\n",
    "- Dropout\n",
    "  - Randomly clamps a subset of hidden units to zero at each iteration of SGD. \n",
    "    - Encourages weights to have smaller magnitudes\n",
    "- Applying noise\n",
    "  - Adding noise to input data\n",
    "  - Adding noise to weights\n",
    "  - Perturb labels\n",
    "    - Randomly changing labels at each training iteration\n",
    "      - Changing loss function to minimize cross entropy between predicted distribution and a distribution where the true label has probability $1-\\rho$ of ocurring, and the other classes have equal probability\n",
    "- Transfer Learning\n",
    "  - Pre-trained to perform a related secondary task for which data are more plentiful\n",
    "  - Resulting model is adapted to the original task\n",
    "  - Can be done\n",
    "    - Removing the last layer and  adding one or more layers that produce a suitable output\n",
    "    - Main model fixed and the new last layers are trained\n",
    "    - Trained end-to-end\n",
    "- Self-supervised learning\n",
    "  - Generative self-supervised learning\n",
    "    - Part of each data example is masked and the secondary task is to predict the missing part\n",
    "  - Contrastive self-supervised learning\n",
    "    - Pair of examples with commonalities are compared to uncorrelated pairs\n",
    "    - For images, might be if two images are transformed versions of one another or unconnected\n",
    "- Augmentation\n",
    "  - Expand the dataset changing each datapoint, but the label stays the same\n",
    "  - Text: Translate to another language and translate it back"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
